{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "We started this project by exploring the dataset. When we first looked through the data, the most surprising thing was the lack of features. The only data columns provided were PhraseId, SentenceId, Phrase, and Sentiment. We soon realized that we would have to generate all of the features used to train a model by using natural language processing tools.\n",
    "\n",
    "The distribution of sentiments was also interesting. There were five categories of sentiments ranging from 0 (negative) to 4 (positive). Even so, the neutral sentiment value occurred more than half the time in the reviews. This means that if we always predict a neutral sentiment, then we will be right about 50% of the time.\n",
    "\n",
    "Finally, our exploration showed us that our intuition about the sentiment of individual words is somewhat accurate. When we looked at which words were most frequent in each type of review, we could see that the positive reviews contained mostly words with a positive connotation and negative reviews contained words with mostly negative connotations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "![Ensemble Diagram](learn/ensemble-diagram.png)\n",
    "\n",
    "In this project, we essentially have 4 separate individual models: word2vec, sentiment, pos-sentiment-ngrams and LDA. The input for each of them comes from the lemmatization data in which we go through all the reviews in the data and replace the words with their lemmatization words. \n",
    "\n",
    "### Parts of Speech\n",
    "\n",
    "For pos-sentiment-ngrams, we combined part of speech, sentiment analysis and ngrams techniques all together. We extracted adjectives and verbs from each reviews, and used countvectorizer with nagrams to build feature matrix of them then weighted the frequencies by their sentiment scores. \n",
    "\n",
    "![pos graph](learn/pos-cv.png)\n",
    "\n",
    "We have some cross validation in notebook, pos-sentiment_cross_validation1. By the bar graph at the end of the notebook, we can see that the performance of the model with this data preprocessing is better than the model’s performance without it. Also we were hesitating that after getting the matrix of verbs and adjectives, if we should combine them with the raw data feature matrix and then use all of them to train model or just simply use the matrix of verbs and adjectives to train model. \n",
    "\n",
    "![pos graph 2](learn/pos-cv2.png)\n",
    "\n",
    "By the bar graph at the end of notebook, pos-sentiment_cross_validation2, we compare the performance of both of these approaches, it seems like from the graph, just using the verbs and adjectives matrix is better for the model to perform.\n",
    "\n",
    "### TextBlob Sentiment\n",
    "\n",
    "Our sentiment model relies heavily on the textblob library. For this model we decided to simply use the sentiment value produced by textblob’s analysis of each phrase. These generated sentiments were fed into a learner as the sole feature and then used to predict output sentiments. The performance of this model was not very impressive. Upon inspecting the sentiments produced by textblob, we noticed that a great deal were neutral. This caused the learner to predict a neutral class about 80% of the time.\n",
    "\n",
    "### Google Word2Vec\n",
    "\n",
    "Google’s Word2Vec library provided an effective way to extract data from the reviews. The tool is able to learn the relationship between many words and express that relationship as a vector. Our initial attempts using Word2Vec give us great performance. This was because the model is much more effective when trained with a large corpus. We then tried again by incorporating another dataset from kaggle to train Word2Vec. This was actually quite effective and boosted performance significantly.\n",
    "\n",
    "### Latent Dirichlet Allocation\n",
    "\n",
    "For our final attempt at data extraction we used the Latent Dirichlet Allocation. LDA is able to take phrases and vectorize them in terms of how often certain topics occur. Instead of recording the frequency of each individual word, this tool lets us find the frequency of related words. This tool provided us with a way to reduce the dimensionality of our data.\n",
    "\n",
    "### Ensemble\n",
    "\n",
    "For our ensemble models, as we mentioned earlier, for each model, we divide the train data into two halves, one for the individual model to train and then use the another half for the ensemble model to analyze the individual model’s accuracy. So every model will render two predictions to the ensemble model, the first one is the prediction from the half of the train data which we know the correct answer sheet already, the another one is the final prediction from the real test data. The ensemble model is simply a logistic regression model. We gather together all the prediction from each model of the half train data as a training data set for this model to learn and the answer sheet will be the y value for validation. So the model will know how to combine the predictions from all models to get the correct y value. Then we give all the final predictions from the real test data of every model to the ensemble model again, then it will give us the combined prediction answer. Based on the testing, we prove that based on current limited data, the ensemble model can perform better than any of the individual branch models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Tools\n",
    "\n",
    "**what tools worked best or were most useful?**\n",
    "    \n",
    "Besides panda and sklearn (of course!), the most useful tool in this project will be textblob, it is built based on some giant libraries like nltk, patterns and wordnet and it has very nice documentation and simple APIs. It inherits all the major nlp functions much easier for user to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "**what could be done to improve performance?**\n",
    "\n",
    "Inside the ensemble folder, there is also another script adopted from an online forum we found which uses a log loss function to find best weights for multi models. If possible, we also want to explore with that code later.\n",
    "\n",
    "Word2Vec is essentially a pretrained model, google has published its own pretrained word2vec which has already been trained with tons of literature data. We tried to run it before in supercomputer but failed eventually due to some dependencies problem. If possible, we really want to combine that into our ensemble model sets.\n",
    "\n",
    "Similar to word2vec’s deep learning theory, we were also thinking of using neural networks to analyze the data potentially. We estimate that mutli nodes and connection of neural networks should be a good model structure for this kind of task.\n",
    "\n",
    "\n",
    "**what other tools could be used?**\n",
    "\n",
    "Some more professional core tools from Natural Language Processing Toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
