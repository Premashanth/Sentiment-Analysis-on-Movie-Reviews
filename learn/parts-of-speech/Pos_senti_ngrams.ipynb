{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech + Sentiment Analysis + Ngrams Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we extract verbs and adjectives out from each review entry and get sentiment analysis of them. We use count vectorizer to build feacture matrix base on these verbs and adjectives with ngrams function. Then we weight them with their sentiment scores before we finally give the data to SVC model to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "train = pd.read_csv('../Lemmatization/result_train.csv')\n",
    "test = pd.read_csv('../Lemmatization/result_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data has been lemmatized, some of the entries may contain nothing anymore and this could potentially cause troubles difficult for debugging later. So we are using fillna to filter all the nan entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.fillna(\" \", inplace = True)\n",
    "test.fillna(\" \", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 3)\n",
      "(66292, 2)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are thinking of making an ensemble model in the end, we split the known data into two groups. One for the current model trainning and the another for the ensemble model to analyze how accurate this model is. By comparing the ensemble knows how to assign individual model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train[:10]\n",
    "# test = test[:10]\n",
    "\n",
    "split_indexs = int(len(train)/2)\n",
    "train_test = train[: split_indexs]\n",
    "train_train = train[split_indexs:]\n",
    "columns = ['PhraseId', 'Phrase', 'Sentiment']\n",
    "train_test= pd.DataFrame(train_test, columns=columns)\n",
    "train_train= pd.DataFrame(train_train, columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combind all the train and test data together. This is prepared to find the same unique words feature vectors from the vectorization function provided by sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 2)\n",
      "(66292, 2)\n",
      "(222352, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "# train_sentiment = train[:,-1]\n",
    "train = train[:, :2]\n",
    "\n",
    "print train.shape\n",
    "print test.shape\n",
    "\n",
    "data = np.concatenate((train, test), axis=0)\n",
    "print data.shape\n",
    "columns = ['Phrase', 'PhraseId']\n",
    "data= pd.DataFrame(data, columns=columns)\n",
    "test = pd.DataFrame(test, columns=columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exract adj and vebvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sentence, generally the verbs and the adjectives are the most important words in that sentence to carry out the sentiment feeling and meaning of the whole sentence. So here we think of using textblob library to extract adjectives and verbs and analyze their sentiments individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "def extract_verbs_and_adj(phrase):\n",
    "    words = []\n",
    "    \n",
    "    tags = TextBlob(phrase).tags\n",
    "    for tag in tags:\n",
    "        if tag[1][:2] == \"VB\" or tag[1][:2] == \"JJ\":\n",
    "            words.append(tag[0])\n",
    "    return words\n",
    "\n",
    "def looping_extract_verbs_and_adj(data_set):\n",
    "    phrases_vb_adj = []\n",
    "\n",
    "    for (index, phrase) in enumerate(data_set):\n",
    "        if index % 10 == 0:\n",
    "            print index\n",
    "        phrases_vb_adj.append(' '.join(extract_verbs_and_adj(phrase)))\n",
    "    return phrases_vb_adj\n",
    "\n",
    "data_vb_adj = looping_extract_verbs_and_adj(data.Phrase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the count vectorizer to get the unique word matrix of the whole data set including both train and test data.\n",
    "Here, we specify ngram_range=(0, 4) parameter for the countvectorizer so it will select all combinations among verbs and adjectives for each review entry ranging from 1 to 4 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v1 = CountVectorizer(ngram_range=(0, 4),  min_df=2, max_df=0.95, max_features=1000)\n",
    "v1.fit(data_vb_adj)\n",
    "\n",
    "\n",
    "train_test_vb_adj = looping_extract_verbs_and_adj(train_test.Phrase)\n",
    "train_train_vb_adj = looping_extract_verbs_and_adj(train_train.Phrase)\n",
    "test_vb_adj = looping_extract_verbs_and_adj(test.Phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the verbs and adjectives, we want to weight the frequencies of the words by their individual sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhecan/anaconda2/lib/python2.7/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "def add_vb_adj_sentiment(phrases_vb_adj):\n",
    "    X2 = v1.transform(phrases_vb_adj)\n",
    "\n",
    "    sentiments = []\n",
    "    for feature in v1.get_feature_names():\n",
    "        sentiment = TextBlob(feature).sentiment\n",
    "        value = sentiment.polarity * sentiment.subjectivity\n",
    "        sentiments.append(value)\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    X3 = csr_matrix(X2.shape)\n",
    "    for index in range(X2.shape[0]):\n",
    "        row = X2[index]\n",
    "        for col in range(row.shape[1]):\n",
    "            X3[index, col] = X2[index, col] * sentiments[col]\n",
    "\n",
    "    return X3\n",
    "\n",
    "X_train_test = add_vb_adj_sentiment(train_test_vb_adj)\n",
    "X_train_train = add_vb_adj_sentiment(train_train_vb_adj)\n",
    "X_test = add_vb_adj_sentiment(test_vb_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 9)\n",
      "(5, 9)\n",
      "(10, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print X_train_train.shape\n",
    "print X_train_test.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let the support vector classifier model to be trained base on the train_train data set and make predictions on both the train_test and test data sets. So later the ensemble model will use the train_test data set's prediction to determine how accurate this model is and then assign appropriate weights for its predictions on test data set for final combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "# y = np.array([1,2,3,4,5])\n",
    "clf.fit(X_train_train, train_train.Sentiment) \n",
    "\n",
    "y_train_test_predict = clf.predict(X_train_test)\n",
    "y_test_predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "train_output = pd.DataFrame({\n",
    "    'PhraseId': train_test.PhraseId,\n",
    "    'Predicted': y_train_test_predict,\n",
    "    'Sentiment': train_test.Sentiment\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "test_output = pd.DataFrame({\n",
    "    'PhraseId': test.PhraseId,\n",
    "    'Sentiment': y_test_predict\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
