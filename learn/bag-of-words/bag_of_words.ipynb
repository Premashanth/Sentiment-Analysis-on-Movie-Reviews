{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "In this notebook, I'll be attempting to predict movie review sentiments using a bag of words model. This model does not look at the context of words in reviews, only their presence and frequency. Hopefully, by classifying reviews based on features corresponding to the frequency of each word will yield somewhat accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('../../data/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('../../data/test.tsv', sep='\\t')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "\n",
    "Our first attempt at feature extraction will be using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from [scikit-learn](http://scikit-learn.org/stable/index.html). This tool will look at each review and count the number of times each word occurs. The number of times a word appears in a review will hopefully correlate with the sentiment of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "subset = train[:200]\n",
    "\n",
    "# Create the vectorizer\n",
    "# We ignoring common english words and only look at a maximum of 2000 unique words\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=2000)\n",
    "X = vectorizer.fit_transform(subset.Phrase).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Now that we've extracted our features, we can learn from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.61\n",
      "dt: 2.040056\n",
      "AdaBoost:      0.66\n",
      "dt: 0.184423\n",
      "SVC:           0.66\n",
      "dt: 0.012639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500)\n",
    "boost = AdaBoostClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "forest_score = cross_val_score(forest, X, subset.Sentiment).mean()\n",
    "print \"Random Forest: %2.2f\" % forest_score\n",
    "print \"dt: %f\" % (time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "boost_score = cross_val_score(boost, X, subset.Sentiment).mean()\n",
    "print \"AdaBoost:      %2.2f\" % boost_score\n",
    "print \"dt: %f\" % (time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_score = cross_val_score(svc, X, subset.Sentiment).mean()\n",
    "print \"SVC:           %2.2f\" % svc_score\n",
    "print \"dt: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "Vectorization only using the frequency of each word did not perform very well. By incorporating the inverse document frequency, we might better evaluate the significance of words in each review. This works by comparing the frequency of each word with the number of times it appears within all the reviews. If every review contains a certain word, then that word probably has a lower significance. This vectorization would reflect that. We will use the [TF-IDF Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from [scikit-learn](http://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "subset = train[:200]\n",
    "\n",
    "# Create the vectorizer\n",
    "# We ignoring common english words and only look at a maximum of 2000 unique words\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(subset.Phrase).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.61\n",
      "dt: 2.058766\n",
      "AdaBoost:      0.62\n",
      "dt: 0.191025\n",
      "SVC:           0.66\n",
      "dt: 0.012861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500)\n",
    "boost = AdaBoostClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "forest_score = cross_val_score(forest, X, subset.Sentiment).mean()\n",
    "print \"Random Forest: %2.2f\" % forest_score\n",
    "print \"dt: %f\" % (time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "boost_score = cross_val_score(boost, X, subset.Sentiment).mean()\n",
    "print \"AdaBoost:      %2.2f\" % boost_score\n",
    "print \"dt: %f\" % (time.time() - t0)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_score = cross_val_score(svc, X, subset.Sentiment).mean()\n",
    "print \"SVC:           %2.2f\" % svc_score\n",
    "print \"dt: %f\" % (time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
